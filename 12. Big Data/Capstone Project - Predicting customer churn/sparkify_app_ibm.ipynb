{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import IntegerType, StringType, DateType\n",
    "from pyspark.sql.functions import isnan, when, count, col, isnull, desc, asc, udf, max, sum, mean, countDistinct, datediff\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+------------------+------+-------------+--------------------+------+\n",
      "|           artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|              song|status|           ts|           userAgent|userId|\n",
      "+-----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+------------------+------+-------------+--------------------+------+\n",
      "|    Martin Orford|Logged In|   Joseph|     M|           20| Morales|597.55057| free|  Corpus Christi, TX|   PUT|NextSong|1532063507000|      292|     Grand Designs|   200|1538352011000|\"Mozilla/5.0 (Mac...|   293|\n",
      "|John Brown's Body|Logged In|   Sawyer|     M|           74|  Larson|380.21179| free|Houston-The Woodl...|   PUT|NextSong|1538069638000|       97|             Bulls|   200|1538352025000|\"Mozilla/5.0 (Mac...|    98|\n",
      "|          Afroman|Logged In| Maverick|     M|          184|Santiago|202.37016| paid|Orlando-Kissimmee...|   PUT|NextSong|1535953455000|      178|Because I Got High|   200|1538352118000|\"Mozilla/5.0 (Mac...|   179|\n",
      "|             null|Logged In| Maverick|     M|          185|Santiago|     null| paid|Orlando-Kissimmee...|   PUT|  Logout|1535953455000|      178|              null|   307|1538352119000|\"Mozilla/5.0 (Mac...|   179|\n",
      "|       Lily Allen|Logged In|   Gianna|     F|           22|  Campos|194.53342| paid|          Mobile, AL|   PUT|NextSong|1535931018000|      245|Smile (Radio Edit)|   200|1538352124000|Mozilla/5.0 (Wind...|   246|\n",
      "+-----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+------------------+------+-------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-0d865840-f207-4d2d-8d5f-801326c3f157',\n",
    "    'iam_service_endpoint': 'https://iam.bluemix.net/oidc/token',\n",
    "    'api_key': 'LRE_HtNpx7OdDqgKCz3QdTzffBlKpxeL9VukOVqspiJF'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_3489cab9ef414102879fb22c3f32bd60_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n",
    "# Please read the documentation of 'SparkSession.read()' to learn more about the possibilities to adjust the data loading.\n",
    "# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json\n",
    "\n",
    "data = spark.read.json(cos.url('medium-sparkify-event-data.json', 'sparkify-donotdelete-pr-ljrykmjvegypmg'))\n",
    "data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543705\n",
      "18\n",
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data.count())\n",
    "print(len(data.columns))\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter(data['userId'] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.select(data.userId, data.page)\n",
    "final_df = df.groupby('userId').pivot('page').count().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop('Cancel').withColumnRenamed('Cancellation Confirmation', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature -  total length of all the songs played by each user.\n",
    "total_songs_length = data.filter(data.page == 'NextSong').groupby('userId').agg(sum('length'))\n",
    "\n",
    "final_df = final_df.join(total_songs_length, on = 'userId', how = 'left') \\\n",
    "                   .withColumnRenamed('NextSong', 'Total Songs Played') \\\n",
    "                   .withColumnRenamed('sum(length)', 'Total Songs Length')\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_date = udf(lambda x: datetime.datetime.fromtimestamp(x/1000), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature : Average number of songs played per day by each user\n",
    "temp = data.select(['userId', 'ts', 'page']).filter(data.page == 'NextSong').withColumn('date', get_date('ts'))\n",
    "count_by_days = temp.groupby(['userId', 'date']).count()\n",
    "avg_songs_per_day = count_by_days.groupby('userId').agg(mean('count')).withColumnRenamed('avg(count)', 'Average Songs Per Day')\n",
    "\n",
    "final_df = final_df.join(avg_songs_per_day, on = 'userId', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature : Number of days between registration and last log entry\n",
    "temp = data.select(['userId', 'ts', 'registration'])\n",
    "registration_timestamp = temp.groupby('userId').min('registration')\n",
    "min_timestmp = temp.select(['userId', 'ts']).groupby('userId').min('ts')\n",
    "max_timestmp = temp.select(['userId', 'ts']).groupby('userId').max('ts')\n",
    "\n",
    "days_registered = registration_timestamp.join(min_timestmp, on='userId').join(max_timestmp, on='userId')\n",
    "days_registered = days_registered.withColumn('Registration Date', get_date('min(registration)')) \\\n",
    "                         .withColumn('Max Date', get_date('max(ts)')).withColumn('Min Date', get_date('min(ts)')) \\\n",
    "                         .withColumn('Days Registered',  datediff(col('Max Date'), col('Registration Date'))) \\\n",
    "                         .select(['userId', 'Days Registered'])\n",
    "\n",
    "final_df = final_df.join(days_registered, on = 'userId', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature : Number of days each user has visited the site.\n",
    "temp = data.select(['userId', 'ts', 'page']).withColumn('date', get_date('ts'))\n",
    "count_by_days = temp.groupby(['userId', 'date']).count()\n",
    "days_visited = count_by_days.groupby('userId').agg(count('date')).withColumnRenamed('count(date)', 'Days Visited')\n",
    "\n",
    "final_df = final_df.join(days_visited, on = 'userId', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature : Most recent level of user\n",
    "flag_paid_level = udf(lambda x: 1 if x == 'paid' else 0, IntegerType())\n",
    "\n",
    "levels = data.select(['userId', 'ts', 'level']).orderBy(desc('ts')).dropDuplicates(['userId']).select(['userId', 'level'])\n",
    "'''temp = data.select(['userId', 'ts', 'level'])\n",
    "max_ts = temp.groupby(['userId']).max('ts').withColumnRenamed('max(ts)', 'ts')\n",
    "levels = max_ts.join(temp, on = ['userId', 'ts'], how = 'left').select(['userId', 'level'])'''\n",
    "levels = levels.withColumn('level', flag_paid_level('level'))\n",
    "\n",
    "final_df = final_df.join(levels, on = 'userId', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature : gender\n",
    "flag_male_gender = udf(lambda x: 1 if x == 'M' else 0, IntegerType())\n",
    "levels = data.select(['userId', 'gender']).dropDuplicates(['userId']).withColumn('gender', flag_male_gender('gender'))\n",
    "\n",
    "final_df = final_df.join(levels, on = 'userId', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add feature : userAgent\n",
    "\n",
    "def extract_os_info(input):\n",
    "    if 'Windows' in input:\n",
    "        return 'Windows'\n",
    "    elif 'Linux' in input:\n",
    "        return 'Linux'\n",
    "    elif 'Mac OS X' in input:\n",
    "        return 'Mac OS X'\n",
    "\n",
    "get_os = udf(extract_os_info, StringType())\n",
    "temp_df = data.select('userId', 'userAgent').dropDuplicates(['userId']).withColumn('userAgent', get_os('userAgent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"userAgent\", outputCol=\"userAgentIndex\")\n",
    "temp_df = indexer.fit(temp_df).transform(temp_df)\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=['userAgentIndex'], outputCols=['userAgentVec'])\n",
    "encoded = encoder.fit(temp_df).transform(temp_df).select(['userId', 'userAgentVec'])\n",
    "\n",
    "final_df = final_df.join(encoded, on = 'userId', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(predictions):\n",
    "    '''Evaluates the input predictions. Prints accuracy and f1 score, and returns the confusion matrix.'''\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "    f1_score = evaluator.evaluate(predictions)\n",
    "    print(\"F1 Score: \", f1_score)\n",
    "\n",
    "    confusion_matrix = predictions.groupby(\"label\").pivot(\"prediction\").count().toPandas()\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9069767441860465\n",
      "F1 Score:  0.8932850095640793\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  0.0  1.0\n",
       "0      0   73    1\n",
       "1      1    7    5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modeling\n",
    "training_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n",
    "#validation_df, test_df = rest.randomSplit([0.5,0.5], seed=42)\n",
    "\n",
    "feature_names = final_df.drop('userId', 'label').schema.names\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_names, outputCol=\"features\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "rf = RandomForestClassifier(featuresCol=\"scaledFeatures\", labelCol=\"label\", numTrees=50)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, rf])\n",
    "model = pipeline.fit(training_df)\n",
    "preds = model.transform(test_df)\n",
    "evaluate_prediction(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8953488372093024\n",
      "F1 Score:  0.8832889336885303\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  0.0  1.0\n",
       "0      0   72    2\n",
       "1      1    7    5"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [50, 100, 150]).addGrid(rf.maxDepth,[5, 10, 20]).build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, \n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName=\"f1\"), numFolds=3)\n",
    "cv_model = crossval.fit(training_df)\n",
    "predictions = cv_model.transform(test_df)\n",
    "evaluate_prediction(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
